{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers datasets","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-10T05:25:14.282052Z","iopub.execute_input":"2025-03-10T05:25:14.282272Z","iopub.status.idle":"2025-03-10T05:25:24.733305Z","shell.execute_reply.started":"2025-03-10T05:25:14.282247Z","shell.execute_reply":"2025-03-10T05:25:24.731608Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.10/site-packages (4.49.0)\nCollecting datasets\n  Downloading datasets-3.3.2-py3-none-any.whl (485 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.4/485.4 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/site-packages (from transformers) (24.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from transformers) (3.17.0)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/site-packages (from transformers) (0.21.0)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/site-packages (from transformers) (4.67.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/site-packages (from transformers) (2.0.2)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/site-packages (from transformers) (0.5.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.10/site-packages (from transformers) (0.29.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/site-packages (from transformers) (2024.11.6)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/site-packages (from datasets) (19.0.1)\nCollecting xxhash\n  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/site-packages (from datasets) (2.2.3)\nCollecting multiprocess<0.70.17\n  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting dill<0.3.9,>=0.3.0\n  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting fsspec[http]<=2024.12.0,>=2023.1.0\n  Downloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting aiohttp\n  Downloading aiohttp-3.11.13-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets) (25.1.0)\nCollecting yarl<2.0,>=1.17.0\n  Downloading yarl-1.18.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (319 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting frozenlist>=1.1.1\n  Downloading frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (241 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.9/241.9 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting multidict<7.0,>=4.5\n  Downloading multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.6/124.6 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting aiohappyeyeballs>=2.3.0\n  Downloading aiohappyeyeballs-2.5.0-py3-none-any.whl (15 kB)\nCollecting async-timeout<6.0,>=4.0\n  Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\nCollecting aiosignal>=1.1.2\n  Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\nCollecting propcache>=0.2.0\n  Downloading propcache-0.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (205 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m205.4/205.4 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests->transformers) (3.10)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/site-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/site-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nInstalling collected packages: xxhash, propcache, multidict, fsspec, frozenlist, dill, async-timeout, aiohappyeyeballs, yarl, multiprocess, aiosignal, aiohttp, datasets\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.2.0\n    Uninstalling fsspec-2025.2.0:\n      Successfully uninstalled fsspec-2025.2.0\n  Attempting uninstall: dill\n    Found existing installation: dill 0.3.9\n    Uninstalling dill-0.3.9:\n      Successfully uninstalled dill-0.3.9\nSuccessfully installed aiohappyeyeballs-2.5.0 aiohttp-3.11.13 aiosignal-1.3.2 async-timeout-5.0.1 datasets-3.3.2 dill-0.3.8 frozenlist-1.5.0 fsspec-2024.12.0 multidict-6.1.0 multiprocess-0.70.16 propcache-0.3.0 xxhash-3.5.0 yarl-1.18.3\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\n# Replace 'your-model-id' with the proper model identifier for Qwen2.5-Math-1.5B or your target model.\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-Math-1.5B\")\nmax_context_length = tokenizer.model_max_length\nprint(\"Maximum context length:\", max_context_length)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T05:30:17.617961Z","iopub.execute_input":"2025-03-10T05:30:17.618297Z","iopub.status.idle":"2025-03-10T05:30:33.312543Z","shell.execute_reply.started":"2025-03-10T05:30:17.618262Z","shell.execute_reply":"2025-03-10T05:30:33.311479Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/7.32k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8df7bddf396747c1b6ff189569307310"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7b7d347a7f242f28984ec7bb6e65f10"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"815ba08e393643c486019aea7765a30d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a74c327b54f4a3fbe40d04048ebafc2"}},"metadata":{}},{"name":"stdout","text":"Maximum context length: 131072\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport math\nimport numpy as np\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    Trainer,\n    TrainingArguments,\n    DataCollatorForLanguageModeling,\n)\nfrom datasets import load_dataset, Dataset\n\n# Make sure to have transformers>=4.37.0 and datasets installed\n# pip install transformers datasets\n\n# 1. Load your text dataset from the Kaggle input path\nwith open('/kaggle/input/nbody-data/cleaned.md', 'r', encoding='utf-8') as f:\n    corpus = f.read()\n\n# 2. Load the tokenizer and determine the model's maximum context length\nmodel_name = \"Qwen/Qwen2.5-Math-1.5B\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmax_context_length = tokenizer.model_max_length\nprint(f\"Maximum context length: {max_context_length}\")\n\n# Use a reasonable chunk size for pretraining (8K tokens per chunk)\nchunk_size = 8192  # You can adjust this based on memory constraints\n\n# 3. Tokenize the entire corpus and split it into reasonably sized chunks\ndef prepare_corpus_for_training(corpus, tokenizer, chunk_size):\n    # Tokenize the entire corpus without truncation\n    tokens = tokenizer(corpus, truncation=False, return_tensors=\"np\")[\"input_ids\"][0]\n    \n    # Split the tokens into chunks of size chunk_size\n    total_chunks = math.ceil(len(tokens) / chunk_size)\n    print(f\"Total tokens: {len(tokens)}, Creating {total_chunks} chunks of size {chunk_size}\")\n    \n    chunks = []\n    for i in range(0, len(tokens), chunk_size):\n        chunk = tokens[i:i + chunk_size].tolist()\n        if len(chunk) < chunk_size:  # Pad the last chunk if needed\n            chunk = chunk + [tokenizer.pad_token_id] * (chunk_size - len(chunk))\n        chunks.append({\"input_ids\": chunk})\n    \n    return Dataset.from_list(chunks)\n\n# Create a custom dataset with properly chunked data\nchunked_dataset = prepare_corpus_for_training(corpus, tokenizer, chunk_size)\n\n# 4. Create a data collator that handles the chunked inputs properly\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False,\n    pad_to_multiple_of=8  # For TPU optimization\n)\n\n# 5. Define optimized training arguments for TPU\ntraining_args = TrainingArguments(\n    output_dir=\"./qwen_math_nbody_model\",\n    overwrite_output_dir=True,\n    num_train_epochs=3,                     # More epochs for better learning\n    per_device_train_batch_size=4,          # Reduced to accommodate larger chunks\n    gradient_accumulation_steps=8,          # Increased for effective batch size\n    learning_rate=1e-5,                     # Lower learning rate for stable training\n    weight_decay=0.01,\n    logging_steps=25,                       # More frequent logging\n    save_steps=100,                         # More frequent saving\n    save_total_limit=5,                     # Keep more checkpoints\n    tpu_num_cores=8,                        # Using all cores of TPU v3-8\n    dataloader_drop_last=True,              # Avoids issues with last incomplete batch\n    report_to=[\"tensorboard\"],\n    fp16=True,                              # Mixed precision for efficiency\n)\n\n# 6. Initialize model with settings optimized for TPU\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    use_cache=False,  # Important for training efficiency\n    device_map=\"auto\"  # Let the framework handle device mapping\n)\n\n# 7. Initialize the Trainer with our optimized components\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=chunked_dataset,\n    data_collator=data_collator,\n)\n\n# 8. Start the pretraining process with error handling\ntry:\n    trainer.train()\n    # Save the final model\n    trainer.save_model(\"./qwen_math_nbody_final\")\n    print(\"Training completed successfully!\")\nexcept Exception as e:\n    print(f\"Training interrupted: {e}\")\n    # Save checkpoint even if interrupted\n    trainer.save_model(\"./qwen_math_nbody_checkpoint\")\n    print(\"Emergency checkpoint saved\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}