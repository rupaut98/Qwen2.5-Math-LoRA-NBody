{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers datasets ipywidgets peft bitsandbytes accelerate","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-10T07:08:42.143871Z","iopub.execute_input":"2025-03-10T07:08:42.144070Z","iopub.status.idle":"2025-03-10T07:08:49.781014Z","shell.execute_reply.started":"2025-03-10T07:08:42.144049Z","shell.execute_reply":"2025-03-10T07:08:49.779979Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.3.1)\nRequirement already satisfied: ipywidgets in /usr/local/lib/python3.10/dist-packages (8.1.5)\nRequirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.14.0)\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.2.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.17.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.29.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.12)\nRequirement already satisfied: comm>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (0.2.2)\nRequirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (7.34.0)\nRequirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (5.7.1)\nRequirement already satisfied: widgetsnbextension~=4.0.12 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (4.0.13)\nRequirement already satisfied: jupyterlab-widgets~=3.0.12 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (3.0.13)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\nRequirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.5.1+cu121)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\nRequirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (75.1.0)\nRequirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\nRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (4.4.2)\nRequirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\nRequirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (3.0.48)\nRequirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (2.19.1)\nRequirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\nRequirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\nRequirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\nRequirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets) (0.2.13)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.45.3\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport math\nimport numpy as np\nimport torch\nfrom tqdm.auto import tqdm \nfrom datetime import timedelta\nimport time\nimport gc\nfrom peft import prepare_model_for_kbit_training\n\n# Set memory optimization environment variable\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    Trainer,\n    TrainingArguments,\n    DataCollatorForLanguageModeling,\n    TrainerCallback\n)\nfrom datasets import load_dataset, Dataset\nfrom peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n\n# Free up memory before starting\ntorch.cuda.empty_cache()\ngc.collect()\n\n# 1. Load your text dataset from the Kaggle input path\nwith open('/kaggle/input/nbody-data/cleaned.md', 'r', encoding='utf-8') as f:\n    corpus = f.read()\n\n# 2. Load the tokenizer and determine the model's maximum context length\nmodel_name = \"Qwen/Qwen2.5-Math-1.5B\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmax_context_length = tokenizer.model_max_length\nprint(f\"Maximum context length: {max_context_length}\")\n\n# Use smaller chunks to further reduce memory pressure\nchunk_size = 2048  # Further reduced from 4096\n\n# 3. Tokenize the entire corpus and split it into reasonably sized chunks\ndef prepare_corpus_for_training(corpus, tokenizer, chunk_size):\n    tokens = tokenizer(corpus, truncation=False, return_tensors=\"np\")[\"input_ids\"][0]\n    \n    total_chunks = math.ceil(len(tokens) / chunk_size)\n    print(f\"Total tokens: {len(tokens)}, Creating {total_chunks} chunks of size {chunk_size}\")\n    \n    chunks = []\n    for i in range(0, len(tokens), chunk_size):\n        chunk = tokens[i:i + chunk_size].tolist()\n        if len(chunk) < chunk_size:\n            chunk = chunk + [tokenizer.pad_token_id] * (chunk_size - len(chunk))\n        chunks.append({\"input_ids\": chunk})\n    \n    return Dataset.from_list(chunks)\n\nchunked_dataset = prepare_corpus_for_training(corpus, tokenizer, chunk_size)\n\n# 4. Create a unified progress tracking callback\nclass EnhancedProgressCallback(TrainerCallback):\n    def __init__(self):\n        self.training_start = time.time()\n        self.epoch_start = None\n        self.progress_bar = None\n        self.current_epoch = 0\n        \n    def on_train_begin(self, args, state, control, **kwargs):\n        print(f\"\\n{'='*70}\")\n        print(f\"TRAINING STARTED\")\n        print(f\"{'='*70}\")\n        \n        # Calculate total steps for all epochs\n        self.total_steps = state.max_steps\n        self.steps_per_epoch = len(trainer.train_dataset) // (args.per_device_train_batch_size * \n                                                            args.gradient_accumulation_steps * \n                                                            torch.cuda.device_count())  # Account for multi-GPU\n    \n    def on_epoch_begin(self, args, state, control, **kwargs):\n        self.epoch_start = time.time()\n        self.current_epoch = state.epoch + 1\n        \n        print(f\"\\n{'='*70}\")\n        print(f\"Beginning Epoch {self.current_epoch}/{args.num_train_epochs}\")\n        print(f\"{'='*70}\")\n        \n        # Create a progress bar for this epoch\n        self.progress_bar = tqdm(\n            total=self.steps_per_epoch, \n            desc=f\"Epoch {self.current_epoch}/{args.num_train_epochs}\",\n            position=0\n        )\n        self.last_logged_step = 0\n        \n    def on_epoch_end(self, args, state, control, **kwargs):\n        # Close progress bar\n        if self.progress_bar:\n            self.progress_bar.close()\n        \n        # Calculate epoch time\n        epoch_time = time.time() - self.epoch_start\n        total_time = time.time() - self.training_start\n        \n        print(f\"\\n{'='*70}\")\n        print(f\"Completed Epoch {self.current_epoch}/{args.num_train_epochs}\")\n        print(f\"Epoch time: {timedelta(seconds=int(epoch_time))}\")\n        print(f\"Total training time: {timedelta(seconds=int(total_time))}\")\n        \n        # Estimate remaining time\n        epochs_remaining = args.num_train_epochs - self.current_epoch\n        est_remaining = epoch_time * epochs_remaining\n        print(f\"Estimated time remaining: {timedelta(seconds=int(est_remaining))}\")\n        print(f\"{'='*70}\\n\")\n        \n    def on_log(self, args, state, control, logs=None, **kwargs):\n        if state.is_local_process_zero and logs and self.progress_bar:\n            # Calculate step progress\n            current_step_in_epoch = state.global_step % self.steps_per_epoch\n            if current_step_in_epoch == 0 and state.global_step > 0:\n                current_step_in_epoch = self.steps_per_epoch\n                \n            # Update progress bar to current position\n            self.progress_bar.n = current_step_in_epoch\n            \n            # Add metrics to progress bar\n            postfix_dict = {}\n            if \"loss\" in logs:\n                postfix_dict[\"loss\"] = f\"{logs['loss']:.4f}\"\n            if \"learning_rate\" in logs:\n                postfix_dict[\"lr\"] = f\"{logs['learning_rate']:.2e}\"\n            \n            # Add GPU memory usage\n            try:\n                allocated = torch.cuda.memory_allocated() / (1024 ** 3)\n                postfix_dict[\"GPU\"] = f\"{allocated:.1f}GB\"\n            except:\n                pass\n                \n            self.progress_bar.set_postfix(**postfix_dict)\n            self.progress_bar.update(0)  # Force refresh\n            \n            # Print step details with percentage completed\n            current_overall = state.global_step\n            progress_percent = current_overall / self.total_steps * 100\n            if current_overall % 20 == 0:  # Print every 20 steps\n                print(f\"Step: {current_overall}/{self.total_steps} ({progress_percent:.1f}%) | \"\n                      f\"Loss: {logs.get('loss', 0):.4f} | \"\n                      f\"LR: {logs.get('learning_rate', 0):.2e}\")\n\n# Create data collator\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False\n)\n\n# 5. LoRA-optimized training arguments for continued pretraining\ntraining_args = TrainingArguments(\n    output_dir=\"./qwen_math_nbody_lora\",\n    overwrite_output_dir=True,\n    num_train_epochs=3,\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=16,\n    # Standard learning rate for non-embedding layers\n    learning_rate=1e-4,\n    weight_decay=0.01,\n    logging_steps=10,\n    save_steps=100,\n    save_total_limit=3,\n    dataloader_drop_last=True,\n    report_to=[\"tensorboard\"],\n    fp16=True,\n    logging_first_step=True,\n    gradient_checkpointing=True,\n    logging_dir=\"./logs\",\n    warmup_steps=100,\n    logging_strategy=\"steps\",\n    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n    # Use adamw_hf which better supports parameter groups for decoupled learning rates\n    optim=\"adamw_hf\"\n)\n\n# 6. Load model for examination to identify all module names\nprint(\"Loading model to identify layer structure...\")\ntemp_model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    use_cache=False,\n    torch_dtype=torch.float16,\n    device_map={\"\": 0}  # Load on first GPU only temporarily\n)\n\n# Identify embedding and other linear layers for targeting\nembedding_layers = []\nlinear_layers = []\nfor name, module in temp_model.named_modules():\n    if 'embed' in name.lower() or 'lm_head' in name.lower():\n        embedding_layers.append(name)\n    elif isinstance(module, torch.nn.Linear) and 'embed' not in name.lower() and 'lm_head' not in name.lower():\n        linear_layers.append(name)\n\nprint(f\"Found {len(embedding_layers)} embedding layers: {embedding_layers}\")\nprint(f\"Found {len(linear_layers)} linear layers\")\n\n# Clean up memory\ndel temp_model\ntorch.cuda.empty_cache()\ngc.collect()\n\n# 7. Comprehensive LoRA configuration for continued pretraining\n# Combine both standard target_modules and add embedding layers\n# List adjusted based on Qwen2.5 specific architecture\nlora_config = LoraConfig(\n    r=8,                            # Reduced from 16\n    lora_alpha=16,                  # Reduced from 32\n    target_modules=[                # Only target key modules\n        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \n        \"gate_proj\", \"up_proj\", \"down_proj\", \"wte\", \"lm_head\"\n    ],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=TaskType.CAUSAL_LM,\n)\n\n# 8. Initialize model with optimized settings\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    use_cache=False, # Load in fp16 for further efficiency\n    device_map=\"auto\"\n)\n\n# 9. Apply LoRA to the model\nprint(\"Applying LoRA adapters to model...\")\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()\n\n# 10. Create custom optimizer with decoupled learning rates\ndef get_optimizer_grouped_parameters(model, embedding_lr=1e-5, non_embedding_lr=1e-4):\n    \"\"\"Create parameter groups with different learning rates for embeddings vs other layers.\"\"\"\n    no_decay = [\"bias\", \"LayerNorm.weight\"]\n    embedding_names = [\"wte\", \"lm_head\"]\n    \n    optimizer_grouped_parameters = [\n        # Embedding params with lower learning rate and no weight decay\n        {\n            \"params\": [p for n, p in model.named_parameters() \n                      if any(nd in n for nd in embedding_names) and p.requires_grad],\n            \"lr\": embedding_lr,\n            \"weight_decay\": 0.0,\n        },\n        # Non-embedding params with regular learning rate and weight decay\n        {\n            \"params\": [p for n, p in model.named_parameters() \n                      if not any(nd in n for nd in embedding_names) \n                      and not any(nd in n for nd in no_decay) and p.requires_grad],\n            \"lr\": non_embedding_lr,\n            \"weight_decay\": training_args.weight_decay,\n        },\n        # Non-embedding params with regular learning rate and no weight decay\n        {\n            \"params\": [p for n, p in model.named_parameters() \n                      if not any(nd in n for nd in embedding_names) \n                      and any(nd in n for nd in no_decay) and p.requires_grad],\n            \"lr\": non_embedding_lr,\n            \"weight_decay\": 0.0,\n        },\n    ]\n    return optimizer_grouped_parameters\n\n# 11. Create a custom trainer with decoupled learning rates\nclass CustomPEFTTrainer(Trainer):\n    def create_optimizer(self):\n        \"\"\"Create optimizer with separate learning rates for embedding vs. non-embedding parameters\"\"\"\n        if self.optimizer is None:\n            # Create parameter groups with decoupled learning rates\n            embedding_lr = self.args.learning_rate / 10  # Lower embedding LR (10x smaller)\n            non_embedding_lr = self.args.learning_rate\n            \n            print(f\"Using decoupled learning rates: embedding={embedding_lr}, other={non_embedding_lr}\")\n            \n            optimizer_grouped_parameters = get_optimizer_grouped_parameters(\n                self.model, \n                embedding_lr=embedding_lr,\n                non_embedding_lr=non_embedding_lr\n            )\n            \n            # Create optimizer with parameter groups\n            self.optimizer = torch.optim.AdamW(\n                optimizer_grouped_parameters,\n                lr=self.args.learning_rate,\n                betas=(0.9, 0.999),\n                eps=1e-8,\n            )\n        \n        return self.optimizer\n\n# 12. Initialize custom trainer with all optimizations\nprogress_callback = EnhancedProgressCallback()\ntrainer = CustomPEFTTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=chunked_dataset,\n    data_collator=data_collator,\n)\ntrainer.add_callback(progress_callback)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T07:08:49.782384Z","iopub.execute_input":"2025-03-10T07:08:49.782697Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Print training configuration\nprint(f\"\\n{'*'*70}\")\nprint(f\"ENHANCED LORA CONTINUED PRETRAINING CONFIGURATION:\")\nprint(f\"Model: {model_name}\")\nprint(f\"LoRA rank: {lora_config.r}\")\nprint(f\"Epochs: {training_args.num_train_epochs}\")\nprint(f\"Using decoupled learning rates: embedding={training_args.learning_rate/10}, other={training_args.learning_rate}\")\nprint(f\"Batch size: {training_args.per_device_train_batch_size} √ó \"\n      f\"{training_args.gradient_accumulation_steps} steps √ó \"\n      f\"{torch.cuda.device_count()} GPUs = \"\n      f\"{training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps * torch.cuda.device_count()}\")\nprint(f\"Dataset size: {len(chunked_dataset)} chunks of {chunk_size} tokens each\")\nprint(f\"{'*'*70}\\n\")\n\n# Start the training process\nprint(\"\\nüöÄ Starting research-optimized LoRA continued pretraining...\\n\")\ntry:\n    trainer.train()\n    print(\"\\n‚úÖ Training completed successfully!\")\n    # Save the final model\n    print(\"Saving final model...\")\n    model.save_pretrained(\"./qwen_math_nbody_final_lora\")\n    tokenizer.save_pretrained(\"./qwen_math_nbody_final_lora\")\n    print(\"Model saved at ./qwen_math_nbody_final_lora\")\nexcept Exception as e:\n    print(f\"\\n‚ùå Training interrupted: {e}\")\n    # Save checkpoint even if interrupted\n    print(\"Saving emergency checkpoint...\")\n    model.save_pretrained(\"./qwen_math_nbody_checkpoint_lora\")\n    tokenizer.save_pretrained(\"./qwen_math_nbody_checkpoint_lora\")\n    print(\"Emergency checkpoint saved at ./qwen_math_nbody_checkpoint_lora\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}